{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "110d2d2a",
      "metadata": {
        "id": "110d2d2a"
      },
      "source": [
        "\n",
        "# Next-Word Prediction with LSTM (Keras/TensorFlow)\n",
        "\n",
        "This notebook trains a small **LSTM language model** to predict the **next word** given a text prefix.\n",
        "It is self-contained and runs on CPU or GPU (e.g., Google Colab).\n",
        "\n",
        "**What you'll do:**\n",
        "1. Install TensorFlow (if needed)\n",
        "2. Prepare a tiny corpus (you can replace it with your own text)\n",
        "3. Tokenize and create training sequences\n",
        "4. Build and train an LSTM model\n",
        "5. Use `generate_next_words()` to predict continuations with adjustable temperature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4bdec0",
      "metadata": {
        "id": "8f4bdec0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# If running locally and TensorFlow is not installed, uncomment the next line.\n",
        "# In Google Colab this typically isn't required, but it's safe to run.\n",
        "!pip -q install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514b3562",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "514b3562",
        "outputId": "f57fedef-3c3e-4f90-bad8-033fdb3ad3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, random, sys, math, numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6c783a",
      "metadata": {
        "id": "2b6c783a"
      },
      "source": [
        "\n",
        "## 1) Prepare a Text Corpus\n",
        "\n",
        "Replace `corpus_text` with your own dataset for better results. You can paste paragraphs of text or load a file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df8257c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0df8257c",
        "outputId": "eec9de7b-6f92-4e6a-a031-ea79298ecd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length (chars): 593\n",
            "\n",
            "Sample:\n",
            " \n",
            "alice was beginning to get very tired of sitting by her sister on the bank,\n",
            "and of having nothing to do: once or twice she had peeped into the book her sister was reading,\n",
            "but it had no pictures or conversations in it, 'and what is the use of a book,' thought alice 'without pictures or conversation?'\n",
            "so she was considering in her own mind (as well as she could, for the hot day made her feel very  ...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# A small public-domain snippet (Lewis Carroll - Alice in Wonderland, short excerpt)\n",
        "corpus_text = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do: once or twice she had peeped into the book her sister was reading,\n",
        "but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
        "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid),\n",
        "whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies,\n",
        "when suddenly a White Rabbit with pink eyes ran close by her.\n",
        "\"\"\"\n",
        "\n",
        "# (Optional) Load your own text file instead\n",
        "# with open('/path/to/your/text.txt', 'r', encoding='utf-8') as f:\n",
        "#     corpus_text = f.read()\n",
        "\n",
        "corpus_text = corpus_text.lower()\n",
        "print(\"Corpus length (chars):\", len(corpus_text))\n",
        "print(\"\\nSample:\\n\", corpus_text[:400], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0022914",
      "metadata": {
        "id": "f0022914"
      },
      "source": [
        "\n",
        "## 2) Tokenize & Create Sequences\n",
        "\n",
        "We create n-gram sequences where each step predicts the next word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e583c930",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e583c930",
        "outputId": "6cf25187-a838-4ed4-a4e3-e59f106edc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 81\n",
            "Number of sequences: 113\n",
            "Max sequence length: 114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((113, 113), (113, 81))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "# Tokenize\n",
        "tokenizer = Tokenizer(oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts([corpus_text])\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # +1 for padding\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# Convert text to token list\n",
        "tokens = tokenizer.texts_to_sequences([corpus_text])[0]\n",
        "\n",
        "# Build input-output sequences\n",
        "# Example: [w1, w2] -> w3 ; [w1, w2, w3] -> w4; etc.\n",
        "sequences = []\n",
        "for i in range(2, len(tokens)):\n",
        "    seq = tokens[:i]\n",
        "    sequences.append(seq)\n",
        "\n",
        "max_len = max(len(s) for s in sequences)\n",
        "\n",
        "# Pad sequences and split into X (inputs) and y (labels = last token)\n",
        "padded = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "X, y = padded[:, :-1], padded[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "print(\"Number of sequences:\", len(sequences))\n",
        "print(\"Max sequence length:\", max_len)\n",
        "X.shape, y.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4711f877",
      "metadata": {
        "id": "4711f877"
      },
      "source": [
        "\n",
        "## 3) Build the LSTM Model\n",
        "\n",
        "A simple Embedding → LSTM → Dense softmax architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d007cbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "0d007cbf",
        "outputId": "b22e449e-edf0-4f8f-ddbd-daec23d9469f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "embedding_dim = 100\n",
        "lstm_units = 128\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len-1),\n",
        "    LSTM(lstm_units, return_sequences=False),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3878d36b",
      "metadata": {
        "id": "3878d36b"
      },
      "source": [
        "\n",
        "## 4) Train\n",
        "\n",
        "Increase `epochs` for better results (and provide a larger corpus).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d9065b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62d9065b",
        "outputId": "cd1f0aaa-b378-468a-c13c-f5ce971dc5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.0111 - loss: 4.3930\n",
            "Epoch 2/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0784 - loss: 4.3844 \n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1014 - loss: 4.3765\n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1014 - loss: 4.3679\n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0798 - loss: 4.3541\n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0739 - loss: 4.3334 \n",
            "Epoch 7/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0621 - loss: 4.2945\n",
            "Epoch 8/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0843 - loss: 4.2337\n",
            "Epoch 9/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0569 - loss: 4.2066\n",
            "Epoch 10/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0628 - loss: 4.1721\n",
            "Epoch 11/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0739 - loss: 4.1516\n",
            "Epoch 12/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0510 - loss: 4.1029 \n",
            "Epoch 13/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0569 - loss: 4.0843 \n",
            "Epoch 14/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0628 - loss: 4.0353\n",
            "Epoch 15/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0628 - loss: 4.0366 \n",
            "Epoch 16/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0628 - loss: 3.9970\n",
            "Epoch 17/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0628 - loss: 3.9155\n",
            "Epoch 18/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.0465 - loss: 3.8867\n",
            "Epoch 19/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0687 - loss: 3.8481\n",
            "Epoch 20/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0694 - loss: 3.7712 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7267d8",
      "metadata": {
        "id": "8d7267d8"
      },
      "source": [
        "\n",
        "## 5) Predict Next Words\n",
        "\n",
        "Use temperature sampling for creative outputs (higher = more random).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c5b11f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6c5b11f",
        "outputId": "a1ba4331-5c09-48c1-a1e0-1190b2a6a0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: alice was beginning\n",
            "Greedy  : alice was beginning to to to to to to to to\n",
            "Creative: alice was beginning to conversation tired ' her her of do\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def sample_from_probs(probs, temperature=1.0):\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    if temperature <= 0:\n",
        "        # Greedy\n",
        "        return np.argmax(probs)\n",
        "    # Temperature scaling\n",
        "    probs = np.log(probs + 1e-9) / temperature\n",
        "    probs = np.exp(probs) / np.sum(np.exp(probs))\n",
        "    return np.random.choice(len(probs), p=probs)\n",
        "\n",
        "def generate_next_words(seed_text, num_words=5, temperature=0.8):\n",
        "    text = seed_text.lower()\n",
        "    for _ in range(num_words):\n",
        "        seq = tokenizer.texts_to_sequences([text])[0]\n",
        "        seq = pad_sequences([seq], maxlen=max_len-1, padding='pre')\n",
        "        preds = model.predict(seq, verbose=0)[0]\n",
        "        next_id = sample_from_probs(preds, temperature=temperature)\n",
        "        next_word = None\n",
        "        # Map id -> word\n",
        "        for w, idx in word_index.items():\n",
        "            if idx == next_id:\n",
        "                next_word = w\n",
        "                break\n",
        "        if not next_word or next_word == \"<oov>\":\n",
        "            # fallback to greedy if OOV/None\n",
        "            next_id = int(np.argmax(preds))\n",
        "            for w, idx in word_index.items():\n",
        "                if idx == next_id:\n",
        "                    next_word = w\n",
        "                    break\n",
        "        text += \" \" + next_word\n",
        "    return text\n",
        "\n",
        "# Quick test after training\n",
        "seed = \"alice was beginning\"\n",
        "print(\"Seed:\", seed)\n",
        "print(\"Greedy  :\", generate_next_words(seed, num_words=8, temperature=0.0))\n",
        "print(\"Creative:\", generate_next_words(seed, num_words=8, temperature=0.9))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277978a0",
      "metadata": {
        "id": "277978a0"
      },
      "source": [
        "\n",
        "## 6) Tips to Improve\n",
        "- Use a **much larger corpus** (millions of tokens) for meaningful predictions.\n",
        "- Increase **epochs** and **model size** (more LSTM units, stacked layers).\n",
        "- Try **GRU** instead of LSTM for speed.\n",
        "- For modern state-of-the-art results, consider **Transformers**.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}